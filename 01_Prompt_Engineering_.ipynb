{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxq3267o2VXOKJ+bYXcszq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/GenAI/blob/main/01_Prompt_Engineering_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "Prompt engineering refers to the process of designing and refining prompts or instructions given to a machine learning model, particularly in the context of language models like GPT-3.5. It involves crafting the input text in such a way that it elicits the desired response from the model. This can include adjusting the wording, adding context, providing examples, or using specific formatting to guide the model towards producing accurate and relevant outputs.\n",
        "\n",
        "## What is prompt?\n",
        "A prompt in the context of language models, refers to the input provided to the model to generate an output. It can be a text snippet, a question, a command, or any form of input that the model is designed to understand and respond to. The prompt sets the context and guides the model on what type of response is expected.\n",
        "\n",
        "For example, in a language generation task, a prompt could be a sentence or a paragraph that initiates the generation process. In a question-answering task, the prompt could be a question for which the model is expected to provide an answer. The quality and relevance of the prompt play a significant role in determining the accuracy and appropriateness of the model's output.\n",
        "\n",
        "## Why we need promt engineering?\n",
        "Prompt engineering is essential for several reasons:\n",
        "\n",
        "1. **Improving Model Performance:** Crafting well-designed prompts can significantly enhance the performance of language models. By providing clear and contextually relevant input, prompt engineering helps guide the model towards generating more accurate and meaningful outputs.\n",
        "\n",
        "2. **Controlling Output:** Effective prompt engineering allows users to control the output of language models. By carefully designing prompts, users can influence the style, tone, and content of the generated text, making it more suitable for their specific needs.\n",
        "\n",
        "3. **Avoiding Bias and Inaccuracy:** Prompt engineering can help mitigate biases and inaccuracies in model outputs. By framing prompts in a neutral and inclusive manner, users can reduce the risk of the model producing biased or inappropriate content.\n",
        "\n",
        "4. **Customizing Responses:** Different prompts can elicit different responses from a language model. Prompt engineering enables users to customize the responses based on their preferences, making the model more adaptable to diverse applications and contexts.\n",
        "\n",
        "5. **Enhancing User Experience:** Well-crafted prompts lead to better user experiences. Clear and informative prompts make it easier for users to interact with language models, understand the generated output, and achieve their desired outcomes.\n"
      ],
      "metadata": {
        "id": "HY3YH9iuhWdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pros of Prompt Engineering Based applications\n",
        "\n",
        "1. **Improved Model Performance:** Prompt engineering can lead to better performance of language models by providing contextually relevant input, resulting in more accurate and meaningful outputs.\n",
        "\n",
        "2. **Controlled Output:** Users can guide the model's output by crafting well-designed prompts, ensuring that the generated text aligns with their specific needs and requirements.\n",
        "\n",
        "3. **Reduced Bias:** Carefully crafted prompts can help mitigate biases in model outputs, promoting fairness and inclusivity in language generation tasks.\n",
        "\n",
        "4. **Customization:** Prompt engineering allows for customized responses based on different prompts, making the model more adaptable to diverse applications and user preferences.\n",
        "\n",
        "5. **Enhanced User Experience:** Clear and informative prompts contribute to a better user experience, enabling users to interact more effectively with the model and achieve their desired outcomes.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "1. **Complexity:** Crafting effective prompts can be challenging and time-consuming, requiring domain knowledge and experimentation to achieve optimal results.\n",
        "\n",
        "2. **Overfitting:** Overly specific or biased prompts may lead to overfitting, where the model performs well on specific prompts but struggles with generalization to new data.\n",
        "\n",
        "3. **Subjectivity:** The effectiveness of prompts can be subjective, varying based on individual preferences and interpretations of what constitutes a good prompt.\n",
        "\n",
        "4. **Limited Flexibility:** Prompt engineering may limit the flexibility of the model, as it relies heavily on the provided prompts for generating outputs and may not generalize well to diverse inputs.\n",
        "\n",
        "5. **Hallucination**: Model sometime can hallucinate, they mess up output. If they don't know the exact answer they will give some other output.\n",
        "\n",
        "6. **Maintenance:** Prompt engineering-based applications may require ongoing maintenance and updates to adapt to changing user needs and evolving language models.\n",
        "\n",
        "7. **Not upto dated**\n",
        "\n",
        "Overall, while prompt engineering offers significant benefits in improving model performance and user experience, it also poses challenges related to complexity, bias mitigation, and maintenance.\n"
      ],
      "metadata": {
        "id": "cuJiwF4nkOmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How APIs can be helpful?\n",
        "\n",
        "APIs (Application Programming Interfaces) can greatly assist in prompt engineering-based applications in several ways:\n",
        "\n",
        "1. **Access to Language Models:** APIs provide access to powerful language models like GPT-3.5, which can be used for prompt engineering tasks. This access allows developers to leverage advanced AI capabilities without having to build models from scratch.\n",
        "\n",
        "2. **Prompt Generation:** APIs can be used to generate prompts dynamically based on user inputs or application requirements. This enables the creation of tailored prompts that are relevant to specific tasks or contexts.\n",
        "\n",
        "3. **Response Analysis:** APIs can analyze model responses generated based on different prompts. This analysis helps evaluate the effectiveness of prompts in eliciting desired outputs and fine-tune prompt strategies accordingly.\n",
        "\n",
        "4. **Model Integration:** APIs facilitate the integration of language models into existing applications or platforms. This integration streamlines the prompt engineering process and enables seamless interaction with the model.\n",
        "\n",
        "5. **Real-time Feedback:** APIs can provide real-time feedback on model performance, including metrics such as accuracy, relevance, and bias. This feedback loop is essential for iteratively refining prompt strategies and improving overall model outcomes.\n",
        "\n",
        "6. **Scaling:** APIs support scaling prompt engineering workflows by handling multiple requests concurrently. This scalability is crucial for applications that require prompt generation and analysis at scale.\n",
        "\n",
        "7. **Versioning and Updates:** APIs often support versioning and updates, allowing developers to access the latest advancements in language models and prompt engineering techniques. This ensures that applications remain up-to-date and effective over time.\n",
        "\n",
        "Overall, APIs play a vital role in enabling efficient and effective prompt engineering-based applications by providing access to cutting-edge language models, facilitating prompt generation and analysis, supporting model integration, and enabling scalability and continuous improvement."
      ],
      "metadata": {
        "id": "Bp3_IZ1jlqDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing OpenAI Library\n",
        "! pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UMU9Vo9hV4G",
        "outputId": "f8faf0e0-73c7-4886-b7c6-e4b25609b16d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will not get free credits on subsequent accounts. It is only possible for one account.\n"
      ],
      "metadata": {
        "id": "OadIQaxCokfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QffBUtLZZ3OD"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key='s'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">   \n",
        "    response = openai.ChatCompletion.create(\n",
        "    model='',\n",
        "    messages=[\n",
        "        {}\n",
        "    ])\n",
        "\n",
        "\n",
        "Model can be:\n",
        "\n",
        "\n",
        "*   gpt 3.5\n",
        "*   gpt 4\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xEQslA0Ep0dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'gpt-3.5-turbo'"
      ],
      "metadata": {
        "id": "zfHWdVkkm4QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each message in the list has two properties: Role and Content\n",
        "\n",
        "\n",
        "*   Role can take one of three values 'system', 'user', or 'assistant'\n",
        "*   Content contains the text of the message from the role.\n",
        "\n"
      ],
      "metadata": {
        "id": "oO5wQtbtr6NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{'role':'user', 'content':'Tell me Joke'}]\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=message)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6lDBxqpvqt",
        "outputId": "8d1c9063-18a6-4bcd-84c0-03272523658e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9NGTiBzQoxtIoQSBpg7HL8yTAJqGF\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1715331914,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Why did the scarecrow win an award?\\nBecause he was outstanding in his field!\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 11,\n",
            "    \"completion_tokens\": 17,\n",
            "    \"total_tokens\": 28\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "M5CXIJK9uvC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351d46dc-01ad-4d54-c223-747a3ffe65e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the scarecrow win an award?\n",
            "Because he was outstanding in his field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use system as a role. Model will behave like a system throughout the conversation"
      ],
      "metadata": {
        "id": "3ixbtb6IwrbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{'role':'system', 'content':'You are person that Speaks like Imran Khan'},\n",
        "           {'role':'user','content':'Tell me a joke'}]\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=message)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Kf9Bl0wKD6",
        "outputId": "f99b7926-c7d3-4a04-adca-5fe690781ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9NGZYydNZ1qdTY28UZECQX0LnXmNo\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1715332276,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"My friend, there is no need for jokes when the laughter comes from the heart, like the joy of winning a cricket match or the satisfaction of planting a tree. Life itself is a never-ending comedy, with its twists and turns, surprises and setbacks. Let us embrace it with a smile and a resilient spirit, for that is the true essence of living.\"\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 25,\n",
            "    \"completion_tokens\": 72,\n",
            "    \"total_tokens\": 97\n",
            "  },\n",
            "  \"system_fingerprint\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj5ExoZ2xVo3",
        "outputId": "b732180d-75e0-4767-d95c-96a0fedeb966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My friend, there is no need for jokes when the laughter comes from the heart, like the joy of winning a cricket match or the satisfaction of planting a tree. Life itself is a never-ending comedy, with its twists and turns, surprises and setbacks. Let us embrace it with a smile and a resilient spirit, for that is the true essence of living.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt,model='gpt-3.5-turbo'):\n",
        "  message = [{'role':'user', 'content':prompt}]\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=message)\n",
        "  return response.choices[0].message['content']"
      ],
      "metadata": {
        "id": "8oSCqH5Mxa_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='Give me some advice'\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdwBEicqc0Dx",
        "outputId": "3d399e4a-22a3-4821-9a71-840b744dcc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few pieces of advice that may be helpful:\n",
            "\n",
            "1. Stay positive and keep a positive mindset, no matter the challenges you may face.\n",
            "2. Set clear goals and work towards achieving them.\n",
            "3. Take care of your physical and mental health by getting enough sleep, eating well, and exercising regularly.\n",
            "4. Surround yourself with supportive and positive people who lift you up.\n",
            "5. Learn from your mistakes and failures, and use them as opportunities for growth and improvement.\n",
            "6. Stay organized and manage your time effectively to increase productivity.\n",
            "7. Don't be afraid to ask for help or seek advice when needed.\n",
            "8. Practice gratitude and mindfulness to appreciate the present moment and stay grounded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enabling Conversation with ChatGPT API\n"
      ],
      "metadata": {
        "id": "5GDzfL2Vdn1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "def chat(user_input, is_clear=False):\n",
        "\n",
        "  global history\n",
        "  if is_clear:\n",
        "    history = []\n",
        "\n",
        "  input_message = {'role':'user', 'content':user_input}\n",
        "  history.append(input_message)\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=history)\n",
        "\n",
        "  response=response.choices[0].message[\"content\"]\n",
        "  response_message = {'role':'assistant','content':response}\n",
        "  history.append(response_message)\n",
        "  return response"
      ],
      "metadata": {
        "id": "ydhws91PdDOC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= 'Tell me a joke'\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8fpIo2Jpngm",
        "outputId": "99ca49bf-9475-458a-9c59-46bc0e44361f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSd4dnnTrX4f",
        "outputId": "1ba010f8-ef44-4666-8562-fb791ab72a16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.11.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.34.0 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "import openai\n",
        "\n",
        "openai.api_key='#'\n",
        "\n",
        "# Initialize history\n",
        "history = []\n",
        "\n",
        "# Define the main function for your chatbot\n",
        "def chat(user_input, is_clear=False):\n",
        "    global history\n",
        "\n",
        "    if is_clear:\n",
        "        history = []\n",
        "\n",
        "    input_message = {'role': 'user', 'content': user_input}\n",
        "    history.append(input_message)\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=history\n",
        "    )\n",
        "\n",
        "    response = response.choices[0].message[\"content\"]\n",
        "    response_message = {'role': 'assistant', 'content': response}\n",
        "    history.append(response_message)\n",
        "    return response\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"GPT-3 Chatbot\")\n",
        "\n",
        "    # Text input for user input\n",
        "    user_input = st.text_input(\"Enter your message:\")\n",
        "\n",
        "    # Button to submit the user input\n",
        "    if st.button(\"Send\"):\n",
        "        # Call the chat function with user_input\n",
        "        bot_response = chat(user_input)\n",
        "\n",
        "        # Display the bot's response\n",
        "        st.text_area(\"Bot's response:\", bot_response, height=100)\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlURf7SkraB4",
        "outputId": "3df4b87e-7f22-4c72-c3c1-393b2c6702f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNbYI6yv3nw8",
        "outputId": "85971478-2c5a-4781-93f7-643be079b49d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.837s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsNLxRHB3qsQ",
        "outputId": "7d65c054-61df-4402-a987-d827052e56ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.75.77.35\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.15s\n",
            "your url is: https://mighty-news-send.loca.lt\n"
          ]
        }
      ]
    }
  ]
}